package cat

import (
	"context"
	"fmt"
	"sync"

	protomem "github.com/tendermint/tendermint/proto/tendermint/mempool"
	"github.com/tendermint/tendermint/types"
)

// FetchTxsFromKeys is called upon by consensus upon receiving a complete compact block.
// The method iterates through the keys in the compact block. For the transactions it
// already has it adds them to a list. For the transactions that are missing it uses a
// block request to track and retrieve them. Once all transactions are retrieved, it returns
// the complete set to the consensus engine. This can be called multiple times sequentially
// with the  same blockID but is not thread safe.
func (memR *Reactor) FetchTxsFromKeys(ctx context.Context, blockID []byte, compactData [][]byte) ([][]byte, error) {
	if request, ok := memR.blockFetcher.GetRequest(blockID); ok {
		memR.Logger.Debug("tracking existing request for block transactions")
		// we already have a request for this block
		return request.WaitForBlock(ctx)
	}

	txs := make([][]byte, len(compactData))
	missingKeys := make(map[int]types.TxKey, len(compactData))
	hasBitArray := NewBitArray(len(compactData))

	// iterate through the keys to know what transactions we have and what are missing
	for i, key := range compactData {
		txKey, err := types.TxKeyFromBytes(key)
		if err != nil {
			return nil, fmt.Errorf("incorrect compact blocks format: %w", err)
		}
		wtx := memR.mempool.store.get(txKey)
		if wtx != nil {
			txs[i] = wtx.tx
			hasBitArray.Set(i)
		} else {
			missingKeys[i] = txKey
		}
	}
	memR.Logger.Info("fetching transactions from peers", "blockID", blockID, "numTxs", len(txs), "numMissing", len(missingKeys))

	// broadcast what transactions in that block
	// we have.
	memR.broadcastHasBlockTxs(blockID, hasBitArray)

	// Check if we got lucky and already had all the transactions.
	if len(missingKeys) == 0 {
		return txs, nil
	}

	// setup a request for this block and begin to track and retrieve all missing transactions
	request := memR.blockFetcher.NewRequest(
		blockID,
		memR.mempool.Height(),
		missingKeys,
		txs,
	)

	// check if there were any pending block messages from peers that we can now process
	if pending, ok := memR.blockFetcher.PopPendingBitArrays(blockID); ok {
		for peerID, bitArray := range pending {
			peer := memR.ids.GetPeer(peerID)
			if peer == nil {
				// we are no longer connected to the peer
				// that provided this bit array so we ignore.
				continue
			}
			txKeys, err := request.GetMissingKeys(bitArray)
			if err != nil {
				memR.Logger.Error("peer sent us invalid bit array", "peer", peerID, "err", err)
				memR.Switch.StopPeerForError(peer, err)
			}
			for _, txKey := range txKeys {
				memR.processSeenTx(peer, peerID, txKey)
			}
		}
	}

	// Wait for the reactor to retrieve and post all transactions.
	return request.WaitForBlock(ctx)
}

// FetchKeysFromTxs is in many ways the opposite method. It takes a full block generated by the application
// and reduces it to the set of keys that need to be gossiped from one mempool to another nodes mempool
// in order to recreate the full block.
func (memR *Reactor) FetchKeysFromTxs(ctx context.Context, blockID []byte, txs [][]byte) ([][]byte, error) {
	keys := make([][]byte, len(txs))
	for idx, tx := range txs {
		// check if the context has been cancelled
		if ctx.Err() != nil {
			return nil, ctx.Err()
		}
		key := types.Tx(tx).Key()
		keys[idx] = key[:]
		has := memR.mempool.store.has(key)
		if !has {
			// If the mempool provided the initial transactions yet received from
			// consensus a transaction it doesn't recognize, this implies that
			// either a tx was mutated or was added by the application. In either
			// case, it is likely no other mempool has this transaction so we
			// preemptively broadcast it to all other peers
			//
			// We don't set the priority, gasWanted or sender fields because we
			// don't know them.
			wtx := newWrappedTx(tx, key, memR.mempool.Height(), 0, 0, "")
			memR.broadcastNewTx(wtx)
			// For safety we also store this transaction in the mempool (ignoring
			// all size limits) so that we can retrieve it later if needed. Note
			// as we're broadcasting it to all peers, we should not receive a `WantTx`
			// unless it gets rejected by the application in CheckTx.
			//
			// Consensus will have an in memory copy of the entire block which includes
			// this transaction so it should not need it.
			memR.mempool.store.set(wtx)
		}
	}
	// As the proposer we have all the transactions so we gossip a HasBlockTxs message
	// with a full bit array (all 1's) indicating such.
	memR.broadcastHasBlockTxs(blockID, NewFullBitArray(len(txs)))

	// return the keys back to the consensus engine
	return keys, nil
}

// broadcastHasBlockTxs send a HasBlockTxs message to all peers to let them know
// which txs in the block we have. Other peers will do the same so we know who
// to fetch what transactions from.
func (memR *Reactor) broadcastHasBlockTxs(blockID []byte, bitArray *BitArray) {
	msg := &protomem.Message{
		Sum: &protomem.Message_HasBlockTxs{
			HasBlockTxs: &protomem.HasBlockTxs{
				BlockId:     blockID,
				HasBitArray: bitArray.Bytes(),
			},
		},
	}
	bz, err := msg.Marshal()
	if err != nil {
		panic(err)
	}

	memR.Logger.Info("broadcasting HasBlockTxs message to all connected peers", "bitArray", bitArray.Bytes())
	for _, peer := range memR.ids.GetAll() {
		peer.Send(MempoolStateChannel, bz)
	}
}

type blockFetcher struct {
	// mutex to manage concurrent calls to different parts
	mtx sync.Mutex
	// requests are a map of all processing block requests
	// by blockID.
	requests map[string]*blockRequest

	// pending is a map of unprocessed HasBlockTx messages
	// The format is the blockID/peerID/bitArray
	pending map[string]map[uint16][]byte

	// the height we first saw a HasBlockTx bit array from
	// a peer. This is used for garbage collection
	pendingHeight map[string]int64
}

// NewBlockFetcher returns a new blockFetcher for managing block requests
func NewBlockFetcher() *blockFetcher {
	return &blockFetcher{
		requests:      make(map[string]*blockRequest),
		pending:       make(map[string]map[uint16][]byte),
		pendingHeight: make(map[string]int64),
	}
}

func (bf *blockFetcher) GetRequest(blockID []byte) (*blockRequest, bool) {
	bf.mtx.Lock()
	defer bf.mtx.Unlock()
	request, ok := bf.requests[string(blockID)]
	return request, ok
}

func (bf *blockFetcher) PopPendingBitArrays(blockID []byte) (map[uint16][]byte, bool) {
	bf.mtx.Lock()
	defer bf.mtx.Unlock()
	request, ok := bf.pending[string(blockID)]
	delete(bf.pending, string(blockID))
	return request, ok
}

// NewRequest creates a new block request and returns it.
// If a request already exists it returns that instead
func (bf *blockFetcher) NewRequest(
	blockID []byte,
	height int64,
	missingKeys map[int]types.TxKey,
	txs [][]byte,
) *blockRequest {
	bf.mtx.Lock()
	defer bf.mtx.Unlock()
	if request, ok := bf.requests[string(blockID)]; ok {
		return request
	}
	request := NewBlockRequest(height, missingKeys, txs)
	bf.requests[string(blockID)] = request
	return request
}

// TryAddMissingTx loops through all current requests and tries to add
// the given transaction (if it is missing).
func (bf *blockFetcher) TryAddMissingTx(key types.TxKey, tx []byte) {
	bf.mtx.Lock()
	defer bf.mtx.Unlock()
	for _, request := range bf.requests {
		request.TryAddMissingTx(key, tx)
	}
}

// PruneOldRequests removes any requests that are older than the given height.
func (bf *blockFetcher) PruneOldRequests(height int64) {
	bf.mtx.Lock()
	defer bf.mtx.Unlock()
	for blockID, request := range bf.requests {
		if request.height <= height {
			delete(bf.requests, blockID)
		}
	}
	for blockID, height := range bf.pendingHeight {
		if height <= height {
			delete(bf.pending, blockID)
			delete(bf.pendingHeight, blockID)
		}
	}
}

// AddPendingBitArray persists a bit array from a peer for a block that we
// don't yet know about. We record the height so we can remove it after
// a while. If the block is eventually requested we then begin to
// process these bit arrays.
func (bf *blockFetcher) AddPendingBitArray(
	blockID, bitArray []byte,
	peerID uint16, height int64,
) {
	bf.mtx.Lock()
	defer bf.mtx.Unlock()
	// it's possible that the block request arrived in
	// between checking and calling `AddPendingBitArray`.
	// This should be rare and in that case we just ignore.
	if bf.requests[string(blockID)] != nil {
		return
	}
	peerInfo, ok := bf.pending[string(blockID)]
	if !ok {
		bf.pending[string(blockID)] = map[uint16][]byte{
			peerID: bitArray,
		}
		bf.pendingHeight[string(blockID)] = height
	} else {
		peerInfo[peerID] = bitArray
	}
}

// blockRequests handle the lifecycle of individual block requests.
type blockRequest struct {
	// immutable fields
	height int64
	doneCh chan struct{}

	mtx sync.Mutex
	// track the remaining keys that are missing
	missingKeysByIndex map[int]types.TxKey
	missingKeys        map[string]int
	// the txs in the block
	txs [][]byte
}

func NewBlockRequest(
	height int64,
	missingKeys map[int]types.TxKey,
	txs [][]byte,
) *blockRequest {
	mk := make(map[string]int, len(missingKeys))
	for i, key := range missingKeys {
		mk[key.String()] = i
	}
	return &blockRequest{
		height:             height,
		missingKeysByIndex: missingKeys,
		missingKeys:        mk,
		txs:                txs,
		doneCh:             make(chan struct{}),
	}
}

// WaitForBlock is a blocking call that waits for the block to be fetched and completed.
// It can be called concurrently. If the block was already fetched it returns immediately.
func (br *blockRequest) WaitForBlock(ctx context.Context) ([][]byte, error) {
	if br.IsDone() {
		return br.txs, nil
	}

	for {
		select {
		case <-ctx.Done():
			return nil, ctx.Err()
		case <-br.doneCh:
			return br.txs, nil
		}
	}
}

// GetMissingKeys takes a bit array (usually the transactions a peer has)
// and returns which ones in the bit array are missing
// so the node knows which ones to request for.
func (br *blockRequest) GetMissingKeys(bitArray []byte) ([]types.TxKey, error) {
	ba, err := NewBitArrayFromBytes(len(br.txs), bitArray)
	if err != nil {
		return nil, err
	}

	txIndices := ba.GetAll()
	keys := make([]types.TxKey, 0, len(txIndices))
	for _, txIndex := range txIndices {
		if key, ok := br.missingKeysByIndex[txIndex]; ok {
			keys = append(keys, key)
		}
	}

	return keys, nil
}

// TryAddMissingTx checks if a given transactions was missing and if so
// adds it to the block request.
func (br *blockRequest) TryAddMissingTx(key types.TxKey, tx []byte) bool {
	br.mtx.Lock()
	defer br.mtx.Unlock()
	if index, ok := br.missingKeys[key.String()]; ok {
		delete(br.missingKeys, key.String())
		delete(br.missingKeysByIndex, index)
		br.txs[index] = tx
		// check if there is any more transactions remaining
		if len(br.missingKeys) == 0 {
			// Yaay! We're done!
			close(br.doneCh)
		}
		return true
	}
	return false
}

// IsDone returns whether all transactions in the block have been received.
// This is done by measuring the amount of missing keys.
func (br *blockRequest) IsDone() bool {
	br.mtx.Lock()
	defer br.mtx.Unlock()
	return len(br.missingKeys) == 0
}

type txAndKey struct {
	tx  []byte
	key types.TxKey
}
